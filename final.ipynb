{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing labreries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the TSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_body</th>\n",
       "      <th>accepted_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"&lt;p&gt;I have this Script that works and does wha...</td>\n",
       "      <td>\"&lt;p&gt;Only rows that are visible in the datatabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;p&gt;I have below code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;*** Setti...</td>\n",
       "      <td>\"&lt;p&gt;If you extract the for loop from the test ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"&lt;p&gt;Anyone know how I can add a smooth transit...</td>\n",
       "      <td>&lt;p&gt;You can add a smooth scroll effect with css...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;p&gt;I want to do something similar to what I ca...</td>\n",
       "      <td>\"&lt;p&gt;Since the initial request to a Blazor Serv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;p&gt;I want to set &lt;code&gt;Build Action&lt;/code&gt; of ...</td>\n",
       "      <td>&lt;pre&gt;&lt;code&gt;&amp;lt;ItemGroup&amp;gt;      &amp;lt;!-- This...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       question_body  \\\n",
       "0  \"<p>I have this Script that works and does wha...   \n",
       "1  <p>I have below code:</p> <pre><code>*** Setti...   \n",
       "2  \"<p>Anyone know how I can add a smooth transit...   \n",
       "3  <p>I want to do something similar to what I ca...   \n",
       "4  <p>I want to set <code>Build Action</code> of ...   \n",
       "\n",
       "                                     accepted_answer  \n",
       "0  \"<p>Only rows that are visible in the datatabl...  \n",
       "1  \"<p>If you extract the for loop from the test ...  \n",
       "2  <p>You can add a smooth scroll effect with css...  \n",
       "3  \"<p>Since the initial request to a Blazor Serv...  \n",
       "4  <pre><code>&lt;ItemGroup&gt;      &lt;!-- This...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"./data/newData.tsv\"\n",
    "\n",
    "df = pd.read_csv(file_path, sep='\\t',usecols=[\"question_body\", \"accepted_answer\"], header=0)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the text data by cleaning, tokenizing, and padding sequences as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the text\n",
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    clean_text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove special characters\n",
    "    clean_text = re.sub(r'[^a-zA-Z\\s]', '', clean_text)\n",
    "    # Convert to lowercase\n",
    "    clean_text = clean_text.lower()\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(clean_text)\n",
    "    clean_words = [word for word in words if word not in stop_words]\n",
    "    # Join the words back into a string\n",
    "    clean_text = ' '.join(clean_words)\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "# Clean the question_body and accepted_answer columns\n",
    "df['question_body'] = df['question_body'].apply(clean_text)\n",
    "df['accepted_answer'] = df['accepted_answer'].apply(clean_text)\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['question_body'] + df['accepted_answer'])\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Convert text to sequences and pad sequences\n",
    "max_length = 100  # Set the maximum sequence length\n",
    "question_sequences = tokenizer.texts_to_sequences(df['question_body'])\n",
    "accepted_answer_sequences = tokenizer.texts_to_sequences(df['accepted_answer'])\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "question_sequences_padded = pad_sequences(question_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "accepted_answer_sequences_padded = pad_sequences(accepted_answer_sequences, maxlen=max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_body</th>\n",
       "      <th>accepted_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>script works need apply first rows next page t...</td>\n",
       "      <td>rows visible datatable actually dom therefore ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>code settings library operatingsystem library ...</td>\n",
       "      <td>extract loop test template test case use templ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anyone know add smooth transition effect tried</td>\n",
       "      <td>add smooth scroll effect css html scrollbehavi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>want something similar react react app renders...</td>\n",
       "      <td>since initial request blazor server app perfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>want set build action two files embedded resou...</td>\n",
       "      <td>ltitemgroupgt lt line includes json files fold...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       question_body  \\\n",
       "0  script works need apply first rows next page t...   \n",
       "1  code settings library operatingsystem library ...   \n",
       "2     anyone know add smooth transition effect tried   \n",
       "3  want something similar react react app renders...   \n",
       "4  want set build action two files embedded resou...   \n",
       "\n",
       "                                     accepted_answer  \n",
       "0  rows visible datatable actually dom therefore ...  \n",
       "1  extract loop test template test case use templ...  \n",
       "2  add smooth scroll effect css html scrollbehavi...  \n",
       "3  since initial request blazor server app perfor...  \n",
       "4  ltitemgroupgt lt line includes json files fold...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 121,   71,   10, ...,    0,    0,    0],\n",
       "       [   2,  652,  233, ...,    0,    0,    0],\n",
       "       [ 499,   65,   33, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 130,   36,  116, ...,    0,    0,    0],\n",
       "       [1988,  172,   27, ...,    0,    0,    0],\n",
       "       [1926,  236,  813, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_sequences_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 169, 1149, 1143, ...,    0,    0,    0],\n",
       "       [ 470,  104,  108, ...,    0,    0,    0],\n",
       "       [  33, 3525,  806, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [1216, 2801,  975, ...,    0,    0,    0],\n",
       "       [  45, 1854,   20, ...,    0,    0,    0],\n",
       "       [ 423,  774,  374, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accepted_answer_sequences_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape: (31529, 100)\n",
      "Validation shape: (7883, 100)\n",
      "Training set shape: (3152900, 1)\n",
      "Validation set shape: (788300, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(question_sequences_padded, accepted_answer_sequences_padded, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training shape:\", X_train.shape)\n",
    "print(\"Validation shape:\", X_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   49,  6656,  5643, ...,     0,     0,     0],\n",
       "       [  177,   827,  1297, ...,     0,     0,     0],\n",
       "       [  275,   228,    40, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [    6,     8,    22, ...,     0,     0,     0],\n",
       "       [  133,   606,   950, ...,     0,     0,     0],\n",
       "       [30026,   745,    51, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Transformer model architecture with encoder and decoder components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the scaled dot product attention.\n",
    "\n",
    "    Args:\n",
    "      q: query shape == (..., seq_len_q, depth)\n",
    "      k: key shape == (..., seq_len_k, depth)\n",
    "      v: value shape == (..., seq_len_v, depth_v)\n",
    "      mask: Float tensor with shape broadcastable \n",
    "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "      output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        print(\"q shape before projection:\", q.shape)\n",
    "        print(\"k shape before projection:\", k.shape)\n",
    "        print(\"v shape before projection:\", v.shape)\n",
    "        print(\"mask shape before projection:\", mask.shape)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        print(\"q shape after projection:\", q.shape)\n",
    "        print(\"k shape after projection:\", k.shape)\n",
    "        print(\"v shape after projection:\", v.shape)\n",
    "        print(\"mask shape after projection:\", mask.shape)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
    "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "        final_output = self.final_layer(dec_output)\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define custom loss function and learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom loss function\n",
    "def custom_loss_function(y_true, y_pred):\n",
    "    # Define your custom loss calculation here\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model with appropriate optimizer and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "# model.compile(optimizer=optimizer, loss=custom_loss_function, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    # Expand dimensions to add padding to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "\n",
    "def create_masks(inp, tar):\n",
    "    # Encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)  # Shape: (batch_size, 1, 1, seq_len_enc)\n",
    "\n",
    "    # Decoder padding mask\n",
    "    dec_padding_mask = create_padding_mask(inp)  # Shape: (batch_size, 1, 1, seq_len_enc)\n",
    "\n",
    "    # Look-ahead mask for decoder self-attention\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])  # Shape: (1, seq_len_tar, seq_len_tar)\n",
    "    dec_target_padding_mask = create_padding_mask(tar)  # Shape: (batch_size, 1, 1, seq_len_tar)\n",
    "\n",
    "    # Combine both masks for decoder self-attention\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)  # Shape: (batch_size, 1, seq_len_tar, seq_len_tar)\n",
    "\n",
    "    # Ensure the correct shape of masks\n",
    "    enc_padding_mask = tf.expand_dims(enc_padding_mask[:, 0, :], axis=1)  # Shape: (batch_size, 1, 1, seq_len_enc)\n",
    "    dec_padding_mask = tf.expand_dims(dec_padding_mask[:, 0, :], axis=1)  # Shape: (batch_size, 1, 1, seq_len_enc)\n",
    "    combined_mask = tf.expand_dims(tf.expand_dims(combined_mask[:, 0, :, :], axis=1), axis=-1)  # Shape: (batch_size, 1, seq_len_tar, seq_len_tar, 1)\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_transformer_model(model, train_dataset, val_dataset, num_epochs, optimizer, loss_function):\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "        for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "            enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar)\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions, _ = model(inp, tar[:, :-1], \n",
    "                                        True, \n",
    "                                        enc_padding_mask, \n",
    "                                        combined_mask, \n",
    "                                        dec_padding_mask)\n",
    "                loss = loss_function(tar[:, 1:], predictions)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            train_loss += loss.numpy()\n",
    "        \n",
    "        for (batch, (inp, tar)) in enumerate(val_dataset):\n",
    "            enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar)\n",
    "            predictions, _ = model(inp, tar[:, :-1], \n",
    "                                    False, \n",
    "                                    enc_padding_mask, \n",
    "                                    combined_mask, \n",
    "                                    dec_padding_mask)\n",
    "            loss = loss_function(tar[:, 1:], predictions)\n",
    "            val_loss += loss.numpy()\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}, Train Loss: {train_loss / len(train_dataset)}, Val Loss: {val_loss / len(val_dataset)}, Time: {time.time() - start} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 10\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "input_vocab_size = target_vocab_size = len(word_index) + 1\n",
    "dropout_rate = 0.1\n",
    "batch_size = 64\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q shape before projection: (64, 100, 128)\n",
      "k shape before projection: (64, 100, 128)\n",
      "v shape before projection: (64, 100, 128)\n",
      "mask shape before projection: (64, 1, 1, 100)\n",
      "q shape after projection: (64, 8, 100, 16)\n",
      "k shape after projection: (64, 8, 100, 16)\n",
      "v shape after projection: (64, 8, 100, 16)\n",
      "mask shape after projection: (64, 1, 1, 100)\n",
      "q shape before projection: (64, 100, 128)\n",
      "k shape before projection: (64, 100, 128)\n",
      "v shape before projection: (64, 100, 128)\n",
      "mask shape before projection: (64, 1, 1, 100)\n",
      "q shape after projection: (64, 8, 100, 16)\n",
      "k shape after projection: (64, 8, 100, 16)\n",
      "v shape after projection: (64, 8, 100, 16)\n",
      "mask shape after projection: (64, 1, 1, 100)\n",
      "q shape before projection: (64, 100, 128)\n",
      "k shape before projection: (64, 100, 128)\n",
      "v shape before projection: (64, 100, 128)\n",
      "mask shape before projection: (64, 1, 1, 100)\n",
      "q shape after projection: (64, 8, 100, 16)\n",
      "k shape after projection: (64, 8, 100, 16)\n",
      "v shape after projection: (64, 8, 100, 16)\n",
      "mask shape after projection: (64, 1, 1, 100)\n",
      "q shape before projection: (64, 100, 128)\n",
      "k shape before projection: (64, 100, 128)\n",
      "v shape before projection: (64, 100, 128)\n",
      "mask shape before projection: (64, 1, 1, 100)\n",
      "q shape after projection: (64, 8, 100, 16)\n",
      "k shape after projection: (64, 8, 100, 16)\n",
      "v shape after projection: (64, 8, 100, 16)\n",
      "mask shape after projection: (64, 1, 1, 100)\n",
      "q shape before projection: (64, 100, 128)\n",
      "k shape before projection: (64, 100, 128)\n",
      "v shape before projection: (64, 100, 128)\n",
      "mask shape before projection: (64, 1, 1, 100)\n",
      "q shape after projection: (64, 8, 100, 16)\n",
      "k shape after projection: (64, 8, 100, 16)\n",
      "v shape after projection: (64, 8, 100, 16)\n",
      "mask shape after projection: (64, 1, 1, 100)\n",
      "q shape before projection: (64, 100, 128)\n",
      "k shape before projection: (64, 100, 128)\n",
      "v shape before projection: (64, 100, 128)\n",
      "mask shape before projection: (64, 1, 1, 100)\n",
      "q shape after projection: (64, 8, 100, 16)\n",
      "k shape after projection: (64, 8, 100, 16)\n",
      "v shape after projection: (64, 8, 100, 16)\n",
      "mask shape after projection: (64, 1, 1, 100)\n",
      "q shape before projection: (64, 100, 128)\n",
      "k shape before projection: (64, 100, 128)\n",
      "v shape before projection: (64, 100, 128)\n",
      "mask shape before projection: (64, 1, 1, 100)\n",
      "q shape after projection: (64, 8, 100, 16)\n",
      "k shape after projection: (64, 8, 100, 16)\n",
      "v shape after projection: (64, 8, 100, 16)\n",
      "mask shape after projection: (64, 1, 1, 100)\n",
      "q shape before projection: (64, 100, 128)\n",
      "k shape before projection: (64, 100, 128)\n",
      "v shape before projection: (64, 100, 128)\n",
      "mask shape before projection: (64, 1, 1, 100)\n",
      "q shape after projection: (64, 8, 100, 16)\n",
      "k shape after projection: (64, 8, 100, 16)\n",
      "v shape after projection: (64, 8, 100, 16)\n",
      "mask shape after projection: (64, 1, 1, 100)\n",
      "q shape before projection: (64, 100, 128)\n",
      "k shape before projection: (64, 100, 128)\n",
      "v shape before projection: (64, 100, 128)\n",
      "mask shape before projection: (64, 1, 1, 100)\n",
      "q shape after projection: (64, 8, 100, 16)\n",
      "k shape after projection: (64, 8, 100, 16)\n",
      "v shape after projection: (64, 8, 100, 16)\n",
      "mask shape after projection: (64, 1, 1, 100)\n",
      "q shape before projection: (64, 100, 128)\n",
      "k shape before projection: (64, 100, 128)\n",
      "v shape before projection: (64, 100, 128)\n",
      "mask shape before projection: (64, 1, 1, 100)\n",
      "q shape after projection: (64, 8, 100, 16)\n",
      "k shape after projection: (64, 8, 100, 16)\n",
      "v shape after projection: (64, 8, 100, 16)\n",
      "mask shape after projection: (64, 1, 1, 100)\n",
      "q shape before projection: (64, 99, 128)\n",
      "k shape before projection: (64, 99, 128)\n",
      "v shape before projection: (64, 99, 128)\n",
      "mask shape before projection: (64, 1, 100, 100, 1)\n",
      "q shape after projection: (64, 8, 99, 16)\n",
      "k shape after projection: (64, 8, 99, 16)\n",
      "v shape after projection: (64, 8, 99, 16)\n",
      "mask shape after projection: (64, 1, 100, 100, 1)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer 'multi_head_attention_250' (type MultiHeadAttention).\n\n{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [64,8,99,99] vs. [64,1,100,100,1] [Op:AddV2] name: \n\nCall arguments received by layer 'multi_head_attention_250' (type MultiHeadAttention):\n  • v=tf.Tensor(shape=(64, 99, 128), dtype=float32)\n  • k=tf.Tensor(shape=(64, 99, 128), dtype=float32)\n  • q=tf.Tensor(shape=(64, 99, 128), dtype=float32)\n  • mask=tf.Tensor(shape=(64, 1, 100, 100, 1), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices((X_val, y_val))\u001b[38;5;241m.\u001b[39mbatch(batch_size)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m train_transformer_model(transformer_model, train_dataset, val_dataset, num_epochs, optimizer, loss_function)\n",
      "Cell \u001b[1;32mIn[92], line 44\u001b[0m, in \u001b[0;36mtrain_transformer_model\u001b[1;34m(model, train_dataset, val_dataset, num_epochs, optimizer, loss_function)\u001b[0m\n\u001b[0;32m     42\u001b[0m enc_padding_mask, combined_mask, dec_padding_mask \u001b[38;5;241m=\u001b[39m create_masks(inp, tar)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 44\u001b[0m     predictions, _ \u001b[38;5;241m=\u001b[39m model(inp, tar[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \n\u001b[0;32m     45\u001b[0m                             \u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[0;32m     46\u001b[0m                             enc_padding_mask, \n\u001b[0;32m     47\u001b[0m                             combined_mask, \n\u001b[0;32m     48\u001b[0m                             dec_padding_mask)\n\u001b[0;32m     49\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_function(tar[:, \u001b[38;5;241m1\u001b[39m:], predictions)\n\u001b[0;32m     50\u001b[0m gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "File \u001b[1;32mc:\\Users\\raidb\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[77], line 12\u001b[0m, in \u001b[0;36mTransformer.call\u001b[1;34m(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n\u001b[0;32m     11\u001b[0m     enc_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(inp, training, enc_padding_mask)\n\u001b[1;32m---> 12\u001b[0m     dec_output, attention_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n\u001b[0;32m     13\u001b[0m     final_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer(dec_output)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m final_output, attention_weights\n",
      "Cell \u001b[1;32mIn[76], line 25\u001b[0m, in \u001b[0;36mDecoder.call\u001b[1;34m(self, x, enc_output, training, look_ahead_mask, padding_mask)\u001b[0m\n\u001b[0;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x, training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[1;32m---> 25\u001b[0m     x, block1, block2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n\u001b[0;32m     27\u001b[0m     attention_weights[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder_layer\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_block1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m block1\n\u001b[0;32m     28\u001b[0m     attention_weights[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder_layer\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_block2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m block2\n",
      "Cell \u001b[1;32mIn[75], line 21\u001b[0m, in \u001b[0;36mDecoderLayer.call\u001b[1;34m(self, x, enc_output, training, look_ahead_mask, padding_mask)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, enc_output, training, look_ahead_mask, padding_mask):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# enc_output.shape == (batch_size, input_seq_len, d_model)\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     attn1, attn_weights_block1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmha1(x, x, x, look_ahead_mask)  \u001b[38;5;66;03m# (batch_size, target_seq_len, d_model)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     attn1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(attn1, training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[0;32m     23\u001b[0m     out1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm1(attn1 \u001b[38;5;241m+\u001b[39m x)\n",
      "Cell \u001b[1;32mIn[69], line 47\u001b[0m, in \u001b[0;36mMultiHeadAttention.call\u001b[1;34m(self, v, k, q, mask)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask shape after projection:\u001b[39m\u001b[38;5;124m\"\u001b[39m, mask\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m scaled_attention, attention_weights \u001b[38;5;241m=\u001b[39m scaled_dot_product_attention(q, k, v, mask)\n\u001b[0;32m     49\u001b[0m scaled_attention \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtranspose(scaled_attention, perm\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m])  \u001b[38;5;66;03m# (batch_size, seq_len_q, num_heads, depth)\u001b[39;00m\n\u001b[0;32m     51\u001b[0m concat_attention \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(scaled_attention, (batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model))  \u001b[38;5;66;03m# (batch_size, seq_len_q, d_model)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[68], line 23\u001b[0m, in \u001b[0;36mscaled_dot_product_attention\u001b[1;34m(q, k, v, mask)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# add the mask to the scaled tensor.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 23\u001b[0m     scaled_attention_logits \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (mask \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1e9\u001b[39m)  \n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# softmax is normalized on the last axis (seq_len_k) so that the scores\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# add up to 1.\u001b[39;00m\n\u001b[0;32m     27\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax(scaled_attention_logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (..., seq_len_q, seq_len_k)\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer 'multi_head_attention_250' (type MultiHeadAttention).\n\n{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [64,8,99,99] vs. [64,1,100,100,1] [Op:AddV2] name: \n\nCall arguments received by layer 'multi_head_attention_250' (type MultiHeadAttention):\n  • v=tf.Tensor(shape=(64, 99, 128), dtype=float32)\n  • k=tf.Tensor(shape=(64, 99, 128), dtype=float32)\n  • q=tf.Tensor(shape=(64, 99, 128), dtype=float32)\n  • mask=tf.Tensor(shape=(64, 1, 100, 100, 1), dtype=float32)"
     ]
    }
   ],
   "source": [
    "pe_input = pe_target = max_length\n",
    "transformer_model = Transformer(num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target)\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size)\n",
    "\n",
    "# Train the model\n",
    "train_transformer_model(transformer_model, train_dataset, val_dataset, num_epochs, optimizer, loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and Prediction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement functions for evaluating the model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function for generating responses to input questions using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
